import numpy as np

class CorrelatedRandomNumberGenerator:
    def __init__(self, num_steps, num_assets, cov_matrix):
        self.num_steps = num_steps  
        self.num_assets = num_assets 
        self.cov_matrix = cov_matrix
         # Cholesky decomposition of the covariance matrix
        # L.T * L = cov_matrix
        self.L = np.linalg.cholesky(self.cov_matrix).T
        self.seed = None

    def generate(self, num_paths, seed=2025): 
        """ Generate relevant standard normally distributed random numbers, Shape of(num_steps, num_assets) """
        np.random.seed(seed)
        self.seed = seed
        Z = np.random.normal(0, 1, (num_paths, self.num_steps, self.num_assets))
        # Converting independent standard normally distributed random numbers into correlated random numbers
        correlated_Z = Z @ self.L

        return correlated_Z

class PathGenerator:
    def __init__(self, S0, r, sigma, T, sim_times, num_assets, random_number_generator):
        self.S0 = S0  
        self.r = r   
        self.sigma = sigma 
        self.T = T    
        self.sim_times = sim_times
        self.dts = np.diff(sim_times) 
        self.num_steps = len(sim_times) - 1
        self.num_assets = num_assets
        self.random_number_generator = random_number_generator

    def generate_paths(self, num_paths):
        """ Time-first: generates prices for all assets in time steps """
        Z = self.random_number_generator.generate(num_paths) 
        paths = np.zeros((num_paths, self.num_steps + 1, self.num_assets))
        paths[:, 0, :] = self.S0
        
        # vectorized computation
        for n in range(1, self.num_steps + 1):
            drift = (self.r - 0.5 * self.sigma ** 2) * self.dts[n-1]
            diffusion = self.sigma * np.sqrt(self.dts[n-1])
            paths[:, n, :] = paths[:, n-1, :] * np.exp(
                (self.r - 0.5 * self.sigma ** 2) * self.dts[n-1] + self.sigma * np.sqrt(self.dts[n-1]) * Z[:, n-1, :]
            )
        
        return paths

# Vectorized conditional probability computation
import numpy as np
from scipy.stats import norm

class RangeAccrualPayoff:
    def __init__(self, S_ref, K, early_termination_times, c1, c2, B_KO, total_days, L, sigma):
        self.S_ref = S_ref
        self.K = K
        self.early_termination_times = early_termination_times
        self.c1 = c1
        self.c2 = c2
        self.B_KO = B_KO
        self.total_days = total_days
        self.daily_Δt = 1/252
        self.L = L  # Simulation interval L
        self.sigma = sigma  # Volatility (can be a constant or a time-dependent function)
        
        # Create sparse time grid
        times = np.linspace(0, 1, total_days + 1)
        self.sim_times = times[::L]
        if times[-1] not in self.sim_times:
            self.sim_times = np.append(self.sim_times, times[-1])
        self.dts = np.diff(self.sim_times)  # Time intervals of sparse grid
        
    def compute(self, paths):
        """Fully vectorized payoff calculation"""
        num_paths, num_timesteps, num_assets = paths.shape
        num_periods = len(self.early_termination_times) - 1
        payoffs = np.zeros((num_paths, num_periods))
    
        performances = paths / self.S_ref
        worst_asset_indices = np.argmin(performances, axis=2)
    
        # Extract the worst asset price paths
        worst_performances = np.zeros((num_paths, num_timesteps))
        for t in range(num_timesteps):
            worst_performances[:, t] = performances[
                np.arange(num_paths), t, worst_asset_indices[:, t]
            ]
        
        # Initialize KO (knock-out) events
        ko_events = np.zeros((num_paths, num_periods), dtype=bool)
        for k in range(1, len(self.early_termination_times)):
            t_k = self.early_termination_times[k]
            idx = np.searchsorted(self.sim_times, t_k, side='right') - 1
            
            # Get the performance of all assets at this time point for all paths
            perf_at_tk = performances[:, idx, :]  # Shape (num_paths, num_assets)
           
            B_KO_k = self.B_KO[k] if isinstance(self.B_KO, list) else self.B_KO
            ko_events[:, k-1] = np.any(perf_at_tk >= B_KO_k, axis=1)
        
        # Carry forward KO events from previous periods
        ko_prev = np.zeros((num_paths, num_periods), dtype=bool)
        for k in range(num_periods):
            if k == 0:
                ko_prev[:, k] = False  # No "previous" KO event in the first period
            else:
                ko_prev[:, k] = np.any(ko_events[:, :k], axis=1)
    
        # Calculate A_k for each observation period
        for k in range(1, len(self.early_termination_times)):
            t_k = self.early_termination_times[k]
            t_k_1 = self.early_termination_times[k-1]
            
            start_idx = np.searchsorted(self.sim_times, t_k_1, side='left')
            end_idx = np.searchsorted(self.sim_times, t_k, side='right') - 1
            
            # Skip paths that are already knocked out
            active_paths = ~ko_prev[:, k-1]
            if not np.any(active_paths):
                continue
            
            # Initialize A_k
            A_k = np.zeros(num_paths)
            
            # Process each sparse time interval
            for j in range(start_idx, end_idx):
                t_start = self.sim_times[j]
                t_end = self.sim_times[j+1]
                
                # Get the worst performance at the current and next time points
                S_jL_worst = worst_performances[active_paths, j]  # Already relative to S_ref
                S_jL_next_worst = worst_performances[active_paths, j+1]
                
                sigma_val = self.sigma(t_start) if callable(self.sigma) else self.sigma
                
                l_values = np.arange(1, self.L + 1)
                
                # Compute all corresponding time points for l
                t_current_values = t_start + l_values * self.daily_Δt
               
                in_observation = (t_current_values >= t_k_1) & (t_current_values <= t_k)
                valid_l = l_values[in_observation]
                
                if len(valid_l) == 0:
                    continue
                
                # Separate cases where l < L and l = L
                l_less = valid_l[valid_l < self.L]
                l_equal = valid_l[valid_l == self.L]
                
                # Vectorized calculation of probabilities for l < L (calculated for all active paths simultaneously)
                if len(l_less) > 0:
                    # Pre-calculate common terms
                    log_ratio = np.log(S_jL_next_worst / S_jL_worst)
                    term1 = self.L * np.log(self.K / S_jL_worst)  # No need for S_ref because it's already relative performance
                    
                    # Vectorized calculation (paths dimension x l dimension)
                    term2 = -l_less * log_ratio[:, None]  # Convert to column vector
                    denominator = np.sqrt(l_less * self.L * (self.L - l_less) * self.daily_Δt) * sigma_val
                    
                    # Handle cases where the denominator is close to zero
                    safe_denom = np.where(np.abs(denominator) < 1e-10, 1e-10, denominator)
                    Z_val = (term1[:, None] + term2) / safe_denom
                    
                    probs = 1 - norm.cdf(Z_val)
                    
                    # Handle special cases where the denominator is zero
                    zero_mask = np.abs(denominator) < 1e-10
                    numerators = term1[:, None] + term2
    
                    # Directly use boolean indexing
                    for i in range(len(l_less)):
                        if zero_mask[i]:
                            # For the i-th column where the denominator is close to zero
                            probs[:, i] = np.where(numerators[:, i] >= 0, 1.0, 0.0)
    
                    A_k[active_paths] += np.sum(probs, axis=1)
                
                # Handle the case where l = L
                if len(l_equal) > 0:
                    l_equal_mask = S_jL_next_worst >= self.K
                    A_k[active_paths] += l_equal_mask
            
            A_k_annual = A_k * self.daily_Δt
            C_k = (1 - A_k_annual/(t_k - t_k_1)) * self.c1 + (A_k_annual/(t_k - t_k_1)) * self.c2
            
            # Set the payoff for KO paths to 0
            C_k[ko_prev[:, k-1]] = 0
            payoffs[:, k-1] = C_k
    
        return payoffs



from tqdm import tqdm

class MonteCarloPricer:
    def __init__(self, path_generator, payoff, num_paths, delta_S=1.0):
        self.path_generator = path_generator  # Path generator
        self.payoff = payoff  # Payoff calculator
        self.num_paths = num_paths  # Number of Monte Carlo paths
        self.delta_S = delta_S

        # Shared parameters extracted from path_generator and payoff
        self.S0 = self.path_generator.S0
        self.r = self.path_generator.r
        self.sigma = self.path_generator.sigma
        self.T = self.path_generator.T
        self.dts = self.path_generator.dts
        self.num_assets = self.path_generator.num_assets
        self.random_number_generator = self.path_generator.random_number_generator
        self.S_ref = self.payoff.S_ref
        self.early_termination_times = self.payoff.early_termination_times
        
    def price(self, paths=None):
        """ Compute present value, optionally using provided paths (used in Greeks calculation) """
        if paths is None:
            paths = self.path_generator.generate_paths(self.num_paths)

        path_payoffs = self.payoff.compute(paths)  # Shape (num_paths, num_periods)
        payoff = np.sum(path_payoffs, axis=1)  # Sum over periods
        pv = np.mean(payoff)
        return pv

    def compute_greeks(self):
        """ Efficiently compute Greeks using path scaling """
        original_paths = self.path_generator.generate_paths(self.num_paths)
        original_paths = np.array(original_paths)

        # Scale paths upward
        scale_factor_up = (self.S0 + self.delta_S) / self.S0
        paths_up = original_paths * scale_factor_up
        pv_up = self.price(paths=paths_up)

        # Scale paths downward
        scale_factor_down = (self.S0 - self.delta_S) / self.S0
        paths_down = original_paths * scale_factor_down
        pv_down = self.price(paths=paths_down)

        pv_base = self.price(paths=original_paths)

        delta = (pv_up - pv_down) / (2 * self.delta_S)
        gamma = (pv_up - 2 * pv_base + pv_down) / (self.delta_S ** 2)
        return delta, gamma
    
    def generate_spot_ladders(self, S0_range):
        """ Generate spot ladder data for PV, Delta, and Gamma """
        pv_ladder = []
        delta_ladder = []
        gamma_ladder = []
        
        original_S0 = self.S0  # Save original spot price
        
        for S in tqdm(S0_range, desc="Generating Spot Ladders"):
            self.path_generator.S0 = S
            self.payoff.S0 = S
            
            pv = self.price()
            delta, gamma = self.compute_greeks()
            
            pv_ladder.append(pv)
            delta_ladder.append(delta)
            gamma_ladder.append(gamma)
        
        # Restore original spot price
        self.path_generator.S0 = original_S0
        self.payoff.S0 = original_S0
        
        return {
            "S0": S0_range,
            "PV": np.array(pv_ladder),
            "Delta": np.array(delta_ladder),
            "Gamma": np.array(gamma_ladder)
        }

    def plot_ladders(ladder_data):
        """ Plot spot ladder graphs """
        plt.figure(figsize=(15, 5))
        
        # PV-Spot
        plt.subplot(1, 3, 1)
        plt.plot(ladder_data["S0"], ladder_data["PV"], "b-o")
        plt.title("PV-Spot Ladder")
        plt.xlabel("Spot Price")
        plt.ylabel("Present Value")
        plt.grid(True)
        
        # Delta-Spot
        plt.subplot(1, 3, 2)
        plt.plot(ladder_data["S0"], ladder_data["Delta"], "r-o")
        plt.title("Delta-Spot Ladder")
        plt.xlabel("Spot Price")
        plt.ylabel("Delta")
        plt.grid(True)
        
        # Gamma-Spot
        plt.subplot(1, 3, 3)
        plt.plot(ladder_data["S0"], ladder_data["Gamma"], "g-o")
        plt.title("Gamma-Spot Ladder")
        plt.xlabel("Spot Price")
        plt.ylabel("Gamma")
        plt.grid(True)
        
        plt.tight_layout()
        plt.show()


S0_range = np.linspace(80, 120, 20)  # Spot price range
num_paths = 1000
delta_S = 1.0
    
correlated_random_number_generator = CorrelatedRandomNumberGenerator(num_steps=num_steps, num_assets=num_assets, cov_matrix=cov_matrix)
path_generator = PathGenerator(S0, r, sigma, T, sim_times, num_assets, correlated_random_number_generator)
payoff = RangeAccrualPayoff(S_ref, K=1.01, early_termination_times=early_termination_times, c1=c1, c2=c2, B_KO=B_KO, total_days=total_days, L=L,sigma=sigma)
monte_carlo_pricer = MonteCarloPricer(path_generator, payoff, num_paths=1000)

ladder_data = monte_carlo_pricer.generate_spot_ladders(S0_range)
graph = plot_ladders(ladder_data)
print(graph)
